{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Software\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\80528\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\c4\\584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db\\c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load a small subset (1000 samples) of the C4 dataset in English\n",
    "dataset = load_dataset(\"c4\", \"en\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset = [x[\"text\"] for _, x in zip(range(10000), dataset) if \"text\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0417064080724db0bade010f9e1faf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top samples have been stored in '3_samples.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize a sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient for semantic tasks\n",
    "\n",
    "# Step 3: Compute embeddings for all sentences\n",
    "embeddings = model.encode(small_dataset, show_progress_bar=True)\n",
    "\n",
    "# Step 4: Calculate semantic differences\n",
    "mean_embedding = np.mean(embeddings, axis=0)  # Average embedding for reference\n",
    "semantic_differences = np.linalg.norm(embeddings - mean_embedding, axis=1)\n",
    "\n",
    "# Step 5: Select top 10 samples with the largest semantic differences\n",
    "top_indices = np.argsort(-semantic_differences)[:3]\n",
    "top_samples = [small_dataset[i] for i in top_indices]\n",
    "\n",
    "# Step 6: Write the top samples to a JSONL file\n",
    "output_file = \"3_samples.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for sample in top_samples:\n",
    "        json.dump({\"text\": sample}, f)\n",
    "        f.write(\"\\n\")  # JSONL format requires newline-separated JSON objects\n",
    "\n",
    "print(f\"Top samples have been stored in '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a07968979f45f4903adbd86501ee4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top samples with IDs have been stored in '100_samples.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize a sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient for semantic tasks\n",
    "\n",
    "# Step 3: Compute embeddings for all sentences\n",
    "embeddings = model.encode(small_dataset, show_progress_bar=True)\n",
    "\n",
    "# Step 4: Calculate semantic differences\n",
    "mean_embedding = np.mean(embeddings, axis=0)  # Average embedding for reference\n",
    "semantic_differences = np.linalg.norm(embeddings - mean_embedding, axis=1)\n",
    "\n",
    "# Step 5: Select top 3 samples with the largest semantic differences\n",
    "top_indices = np.argsort(-semantic_differences)[:100]\n",
    "top_samples = [{\"id\": int(i), \"text\": small_dataset[i]} for i in top_indices]\n",
    "\n",
    "# Step 6: Write the top samples with IDs to a JSONL file\n",
    "output_file = \"100_samples.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for sample in top_samples:\n",
    "        json.dump(sample, f)\n",
    "        f.write(\"\\n\")  # JSONL format requires newline-separated JSON objects\n",
    "\n",
    "print(f\"Top samples with IDs have been stored in '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
