{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb160c6-7d5f-4c60-934b-bb71dea55ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f12e86-d276-4634-8866-bc37e2984211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, they have left an indelible mark on the industry . The birth of Hugging face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf .\n"
     ]
    }
   ],
   "source": [
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary_output = summarizer(\n",
    "    ARTICLE,\n",
    "    max_length=100,      # Adjust max length for summary\n",
    "    min_length=30,       # Adjust min length for summary\n",
    "    do_sample=True,      # Enables sampling for temperature to take effect\n",
    "    temperature=0.7,     # Controls the randomness\n",
    "    top_k=50,            # Limits sampling to top K most likely options\n",
    "    top_p=0.9            # Limits sampling to top cumulative probability P\n",
    ")\n",
    "\n",
    "# Extract the generated summary text\n",
    "summary_text = summary_output[0]['summary_text']\n",
    "print(\"Generated Summary:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56fa27f5-fdcd-4eec-85b2-064a14e3fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"Falconsai/text_summarization\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4072f579-09fa-42b3-863a-f7ca40d5b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: of NLP and AI as a whole Hugging Face has emerged as a prominent and innovative force. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"Falconsai/text_summarization\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Summarize the text:\n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input article and move tensors to the selected device\n",
    "inputs = tokenizer(ARTICLE, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "\n",
    "# Generate summary with controlled parameters\n",
    "summary_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,        # Maximum length of the summary\n",
    "    min_length=30,         # Minimum length of the summary\n",
    "    do_sample=False,       # Disables sampling, for more deterministic output\n",
    "    early_stopping=True,   # Stops early when conditions are met for brevity\n",
    "    length_penalty=1.2,    # Adjusts penalty for longer text to encourage conciseness\n",
    "    num_beams=3            # Enables beam search for more optimal summary generation\n",
    ")\n",
    "\n",
    "# Decode the generated summary text\n",
    "summary_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48a1c55-66c9-416d-9e1c-a1e840c9170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 849, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "# Define a function to extract embeddings\n",
    "def get_text_embeddings(text):\n",
    "    # Tokenize input text and retrieve model outputs\n",
    "    inputs = summarizer.tokenizer(text, return_tensors=\"pt\")\n",
    "    # Use model encoder to get embeddings directly\n",
    "    encoder_outputs = summarizer.model.get_encoder()(inputs[\"input_ids\"])\n",
    "    \n",
    "    # Extract embeddings from the encoder outputs\n",
    "    embeddings = encoder_outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "embeddings = get_text_embeddings(text)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # (batch_size, sequence_length, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98dc605e-eec7-43dc-869b-d6ac55dfab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Text Embeddings Shape: torch.Size([1, 1, 512])\n",
      "Generated Text: ,, and,, and,, and democratized access to AI. Conclusion Hugging Face's journey is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. Conclusion Hugging Face's journey is one of transformation, collaboration, and empowerment. Their commitment to open-source collaboration has reshaped the NLP landscape and democratized access to AI\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, SummarizationPipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 自定义SummarizationPipeline来返回decoder的hidden states\n",
    "class CustomSummarizationPipeline(SummarizationPipeline):\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        # 设置生成参数以确保输出decoder的hidden states\n",
    "        generate_kwargs = {\n",
    "            \"output_hidden_states\": True,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"max_length\": kwargs.pop(\"max_length\", 100),\n",
    "            \"min_length\": kwargs.pop(\"min_length\", 30),\n",
    "        }\n",
    "        \n",
    "        # 使用预处理方法获取模型输入\n",
    "        model_inputs = self.preprocess(*args)\n",
    "        \n",
    "        # 将模型输入移动到模型所在的设备\n",
    "        model_inputs = {key: tensor.to(self.model.device) for key, tensor in model_inputs.items()}\n",
    "        \n",
    "        # 调用生成方法并传递生成参数\n",
    "        outputs = self.model.generate(**model_inputs, **generate_kwargs)\n",
    "        \n",
    "        # 提取生成的token序列和decoder的hidden states\n",
    "        generated_tokens = outputs.sequences  # 提取生成的token序列\n",
    "        decoder_hidden_states = outputs.decoder_hidden_states  # List of tensors for each layer\n",
    "        \n",
    "        # 返回最后一层的decoder hidden states作为最终的text embeddings\n",
    "        final_decoder_embeddings = decoder_hidden_states[-1]  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 将生成的token序列解码为文本\n",
    "        generated_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        return final_decoder_embeddings, generated_text\n",
    "\n",
    "# 初始化tokenizer和模型\n",
    "model_name = \"Falconsai/text_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 使用自定义的SummarizationPipeline\n",
    "custom_summarizer = CustomSummarizationPipeline(model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# 获取生成的decoder text embeddings和文本\n",
    "text = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "decoder_embeddings, generated_text = custom_summarizer(\n",
    "    text, \n",
    "    max_length=100,      # Adjust max length for summary\n",
    "    min_length=30,       # Adjust min length for summary\n",
    "    do_sample=True,      # Enables sampling for temperature to take effect\n",
    "    temperature=0.7,     # Controls the randomness\n",
    "    top_k=50,            # Limits sampling to top K most likely options\n",
    "    top_p=0.9            # Limits sampling to top cumulative probability P)\n",
    ")\n",
    "\n",
    "print(\"Decoder Text Embeddings Shape:\", decoder_embeddings[0].shape)  # (seq_len, hidden_dim)\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f04770-ce49-4823-adf6-e4c5af510a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (849 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: the democratization of Natural Language Processing (NLP) Hugging Face is a prominent and innovative force. From its inception to its role in democratizing AI Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face. Hugging Face has empowered a vibrant online community where developers, researchers, and AI enthusiasts can collaborate and share their knowledge, code, and insights.\n",
      "Generated Token IDs: tensor([    0, 32099,     8,     3, 23319,  1707,    13,  6869, 10509, 19125,\n",
      "           41,   567,  6892,    61, 11560,  3896,  8881,    19,     3,     9,\n",
      "         8304,    11,  3058,  2054,     5,  1029,   165,    16,  7239,    12,\n",
      "          165,  1075,    16,     3, 23319,  2610,  7833, 11560,  3896,  8881,\n",
      "           65, 13999,    38,     3,     9,  8304,    11,  3058,  2054,     5,\n",
      "          100,  1108,    56,  2075,     8,   733,    11, 11978,    13, 11560,\n",
      "         3896,  8881,     5, 11560,  3896,  8881,    65, 25259,     3,     9,\n",
      "         8328,   367,   573,   213,  5564,     6,  4768,     6,    11,  7833,\n",
      "        18872,    54, 11194,    11,   698,    70,  1103,     6,  1081,     6,\n",
      "           11,  7639,     5,     1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device=device)\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = summarizer.tokenizer(ARTICLE, return_tensors='pt')\n",
    "\n",
    "# Move input tensors to the correct device\n",
    "inputs = {key: value.to(summarizer.model.device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate summary and get token IDs\n",
    "summary_ids = summarizer.model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Decode the summary text\n",
    "summary_text = summarizer.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary_text)\n",
    "\n",
    "# Get the embeddings (token IDs) before decoding\n",
    "print(\"Generated Token IDs:\", summary_ids[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d88556f-c3e0-4895-a4ee-fb124aaa5f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (851 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Hugging Face has emerged as a prominent and innovative force in NLP . From its inception to its role in democratizing AI, they have left an indelible mark on the industry . The birth of Hugging face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device=device)\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary_output = summarizer(\n",
    "    ARTICLE,\n",
    "    max_length=100,      # Adjust max length for summary\n",
    "    min_length=30,       # Adjust min length for summary\n",
    "    do_sample=True,      # Enables sampling for temperature to take effect\n",
    "    temperature=0.7,     # Controls the randomness\n",
    "    top_k=50,            # Limits sampling to top K most likely options\n",
    "    top_p=0.9            # Limits sampling to top cumulative probability P\n",
    ")\n",
    "\n",
    "# Extract the generated summary text\n",
    "summary_text = summary_output[0]['summary_text']\n",
    "print(\"Generated Summary:\", summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a6291b-44b7-46a0-b2e5-ba2a157abb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: ,, and more. This article will explore the story and significance of Hugging Face. The Company's mission is to make AI models more accessible and friendly to humans. From its inception to its role in democratizing AI, Hugging Face has emerged as a prominent and innovative force. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry. This article will explore the story\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 67\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Get the embeddings (hidden states) of the generated tokens\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# The last layer's hidden states correspond to the embeddings after the final decoder layer\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Shape: (batch_size, sequence_length, hidden_size)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdecoder_hidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize the summarization pipeline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\", device=device)\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "Hugging Face: Revolutionizing Natural Language Processing\n",
    "Introduction\n",
    "In the rapidly evolving field of Natural Language Processing (NLP), Hugging Face has emerged as a prominent and innovative force. This article will explore the story and significance of Hugging Face, a company that has made remarkable contributions to NLP and AI as a whole. From its inception to its role in democratizing AI, Hugging Face has left an indelible mark on the industry.\n",
    "The Birth of Hugging Face\n",
    "Hugging Face was founded in 2016 by Clément Delangue, Julien Chaumond, and Thomas Wolf. The name \"Hugging Face\" was chosen to reflect the company's mission of making AI models more accessible and friendly to humans, much like a comforting hug. Initially, they began as a chatbot company but later shifted their focus to NLP, driven by their belief in the transformative potential of this technology.\n",
    "Transformative Innovations\n",
    "Hugging Face is best known for its open-source contributions, particularly the \"Transformers\" library. This library has become the de facto standard for NLP and enables researchers, developers, and organizations to easily access and utilize state-of-the-art pre-trained language models, such as BERT, GPT-3, and more. These models have countless applications, from chatbots and virtual assistants to language translation and sentiment analysis.\n",
    "Key Contributions:\n",
    "1. **Transformers Library:** The Transformers library provides a unified interface for more than 50 pre-trained models, simplifying the development of NLP applications. It allows users to fine-tune these models for specific tasks, making it accessible to a wider audience.\n",
    "2. **Model Hub:** Hugging Face's Model Hub is a treasure trove of pre-trained models, making it simple for anyone to access, experiment with, and fine-tune models. Researchers and developers around the world can collaborate and share their models through this platform.\n",
    "3. **Hugging Face Transformers Community:** Hugging Face has fostered a vibrant online community where developers, researchers, and AI enthusiasts can share their knowledge, code, and insights. This collaborative spirit has accelerated the growth of NLP.\n",
    "Democratizing AI\n",
    "Hugging Face's most significant impact has been the democratization of AI and NLP. Their commitment to open-source development has made powerful AI models accessible to individuals, startups, and established organizations. This approach contrasts with the traditional proprietary AI model market, which often limits access to those with substantial resources.\n",
    "By providing open-source models and tools, Hugging Face has empowered a diverse array of users to innovate and create their own NLP applications. This shift has fostered inclusivity, allowing a broader range of voices to contribute to AI research and development.\n",
    "Industry Adoption\n",
    "The success and impact of Hugging Face are evident in its widespread adoption. Numerous companies and institutions, from startups to tech giants, leverage Hugging Face's technology for their AI applications. This includes industries as varied as healthcare, finance, and entertainment, showcasing the versatility of NLP and Hugging Face's contributions.\n",
    "Future Directions\n",
    "Hugging Face's journey is far from over. As of my last knowledge update in September 2021, the company was actively pursuing research into ethical AI, bias reduction in models, and more. Given their track record of innovation and commitment to the AI community, it is likely that they will continue to lead in ethical AI development and promote responsible use of NLP technologies.\n",
    "Conclusion\n",
    "Hugging Face's story is one of transformation, collaboration, and empowerment. Their open-source contributions have reshaped the NLP landscape and democratized access to AI. As they continue to push the boundaries of AI research, we can expect Hugging Face to remain at the forefront of innovation, contributing to a more inclusive and ethical AI future. Their journey reminds us that the power of open-source collaboration can lead to groundbreaking advancements in technology and bring AI within the reach of many.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text using the tokenizer from the pipeline\n",
    "inputs = summarizer.tokenizer(\n",
    "    ARTICLE, \n",
    "    return_tensors='pt', \n",
    "    truncation=True, \n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "# Move inputs to the correct device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate summary and get embeddings using the model from the pipeline\n",
    "# Use the same generation parameters as your original code\n",
    "outputs = summarizer.model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    return_dict_in_generate=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "# Extract the generated summary text\n",
    "summary_ids = outputs.sequences[0]\n",
    "summary_text = summarizer.tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", summary_text)\n",
    "\n",
    "# Get the embeddings (hidden states) of the generated tokens\n",
    "# The last layer's hidden states correspond to the embeddings after the final decoder layer\n",
    "# Shape: (batch_size, sequence_length, hidden_size)\n",
    "embeddings = outputs.decoder_hidden_states[-1]\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8903c9a-0d16-4d5a-b471-c65b67becbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "035d730ba5184a3a83c11a305b47c184": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "047b34b702174bd9a87fbd6627c81bda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9642d48784df43f49680f84cfb16c9df",
       "max": 1493,
       "style": "IPY_MODEL_b718e89c840746d29b049c2d4e1ebd12",
       "value": 1493
      }
     },
     "0566417b609943fd820324db111cecdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5b9a59905c1143578cdbdb0de9c1a118",
       "max": 791656,
       "style": "IPY_MODEL_e8830a7403a84bb88ccb31d88ff525f1",
       "value": 791656
      }
     },
     "05f2eb9e728c42de9ec82edfead8b1a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "066cb1e4ea8e4abc8165470e77427e75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "070dc19b2e384778aada666e56a18e68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ef5dea833d9844298ee4766615390c92",
       "max": 2201,
       "style": "IPY_MODEL_57ead3f715bc41fc87f29b5c4d78b531",
       "value": 2201
      }
     },
     "07a0a776539e40649dd4523c25cd1753": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "097f033939774126990e6e4acb50c0d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6ea2157f4957461ebcfa30aabcd5a517",
       "style": "IPY_MODEL_ee43abee1219410e9d14f1ad94e95eee",
       "value": " 2.32k/2.32k [00:00&lt;00:00, 332kB/s]"
      }
     },
     "09b56baf44ea48aca527ba2919b220ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_035d730ba5184a3a83c11a305b47c184",
       "max": 112,
       "style": "IPY_MODEL_18c8f94ed246413091f84d5ebc1bf325",
       "value": 112
      }
     },
     "11546140f2b14e50a2cc32104a16a36e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1323e22e8af64ffbb78225e570dc4916": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7023c24529a94e64a2b9439ab40c35b9",
       "style": "IPY_MODEL_e5fff6e9c5ad42c98f714ab1c074cc39",
       "value": "generation_config.json: 100%"
      }
     },
     "139513e5cc36491bbea6cbfc3bfa796e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6e184ba5d085455bbc30b0df13aafd93",
       "style": "IPY_MODEL_bc09a7b669f84fe6bdf9f8520243fefa",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "13abe991ef68484d862db5388f759071": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1323e22e8af64ffbb78225e570dc4916",
        "IPY_MODEL_09b56baf44ea48aca527ba2919b220ac",
        "IPY_MODEL_b157804a457e419ebe1df5c9914b288b"
       ],
       "layout": "IPY_MODEL_ae854b3ce06347fa8d79c6eaccaf6685"
      }
     },
     "14a0e957016f44daa432cbb7dac1aa13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "17ac323f7fe04d57a090a4e894d17928": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "18c8f94ed246413091f84d5ebc1bf325": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "32ba6797f61447bfb31b0491f0b44fc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "390d1e92db444034a72dcd6bc03c2e1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ccdd79059c2b4021ae5585e71be7e1b2",
       "style": "IPY_MODEL_6d0f00562a9b43b199fec5d977cbe376",
       "value": " 792k/792k [00:00&lt;00:00, 20.5MB/s]"
      }
     },
     "40b47b569a7446d4a582c1149147b1ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4be9ec3e822f4a8a9df4a00065508cdc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bbf340a4b8104ff69e6d492b73b96894",
        "IPY_MODEL_6824dcb0a45e4311b8aa65b3fc62c7ae",
        "IPY_MODEL_097f033939774126990e6e4acb50c0d0"
       ],
       "layout": "IPY_MODEL_52209ec613b7466cbcef215c9b55b333"
      }
     },
     "52209ec613b7466cbcef215c9b55b333": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "57ead3f715bc41fc87f29b5c4d78b531": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "581761b621664559b7cfeeb0f371bc2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_066cb1e4ea8e4abc8165470e77427e75",
       "max": 242042016,
       "style": "IPY_MODEL_89c9698dfc364cfeadda834835a95115",
       "value": 242042016
      }
     },
     "5b9a59905c1143578cdbdb0de9c1a118": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6824dcb0a45e4311b8aa65b3fc62c7ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d269900a34164b108a7d79b84984d541",
       "max": 2324,
       "style": "IPY_MODEL_86657f75fac04ba39de0943ebc4e709c",
       "value": 2324
      }
     },
     "6d0f00562a9b43b199fec5d977cbe376": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6e184ba5d085455bbc30b0df13aafd93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ea2157f4957461ebcfa30aabcd5a517": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7023c24529a94e64a2b9439ab40c35b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "79e8cd1c11e843cd8ceeb381a620e89e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7b78610739f94c989a2ec61b933d42ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b31084903522470ab45178475d6e9db3",
        "IPY_MODEL_0566417b609943fd820324db111cecdc",
        "IPY_MODEL_390d1e92db444034a72dcd6bc03c2e1a"
       ],
       "layout": "IPY_MODEL_9142b7174aad43b8bd0db6a08ccfb154"
      }
     },
     "7e00c5bf43e641dab96a6032b66e0a6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f1b0faf2fe5b4b4596ed9c662e6d10cd",
       "style": "IPY_MODEL_8a4350ebc19d44c7be07cec9d31dca02",
       "value": "model.safetensors: 100%"
      }
     },
     "81a58f7eed05485ca1caa1fb17a45a5b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "829cfc12875442c39215776ce2e0d65d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_cfd8ab4f693d4c03a55bdae92acfbeb8",
       "style": "IPY_MODEL_96a6aaacb8514744b42c4b5ce2a55848",
       "value": "config.json: 100%"
      }
     },
     "8301a3a0c8344cd1a4f156ffc0e084ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9a6bcea2fac4448aabcad5e7ea4d0507",
       "style": "IPY_MODEL_859ba2746cfd4be2a6b044701d02171a",
       "value": "tokenizer.json: 100%"
      }
     },
     "859ba2746cfd4be2a6b044701d02171a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86657f75fac04ba39de0943ebc4e709c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8737678c87714c3089e627a1b3cae43d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "89c9698dfc364cfeadda834835a95115": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8a4350ebc19d44c7be07cec9d31dca02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9142b7174aad43b8bd0db6a08ccfb154": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "91b2fb36dcea444db6a0d86cd5a2c747": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a81d2db1df0c4cd29fa8190718c6a49d",
       "style": "IPY_MODEL_14a0e957016f44daa432cbb7dac1aa13",
       "value": " 2.42M/2.42M [00:00&lt;00:00, 12.1MB/s]"
      }
     },
     "9642d48784df43f49680f84cfb16c9df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "96a6aaacb8514744b42c4b5ce2a55848": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "980fd3cfe5bc4880ae9a03d69810cdce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98dad05a893e424dbf08d8eb0ae087c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_829cfc12875442c39215776ce2e0d65d",
        "IPY_MODEL_047b34b702174bd9a87fbd6627c81bda",
        "IPY_MODEL_fc28858b8f42414d960b43789b52043f"
       ],
       "layout": "IPY_MODEL_81a58f7eed05485ca1caa1fb17a45a5b"
      }
     },
     "9a6bcea2fac4448aabcad5e7ea4d0507": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e5e5c92ec4a4461b755072cde8ef886": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6cc858b29a44c85a43bbb6d1d3e14f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_7e00c5bf43e641dab96a6032b66e0a6e",
        "IPY_MODEL_581761b621664559b7cfeeb0f371bc2f",
        "IPY_MODEL_c231f4bed0564121acf45efccea5d786"
       ],
       "layout": "IPY_MODEL_980fd3cfe5bc4880ae9a03d69810cdce"
      }
     },
     "a81d2db1df0c4cd29fa8190718c6a49d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ae854b3ce06347fa8d79c6eaccaf6685": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b157804a457e419ebe1df5c9914b288b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_40b47b569a7446d4a582c1149147b1ae",
       "style": "IPY_MODEL_32ba6797f61447bfb31b0491f0b44fc1",
       "value": " 112/112 [00:00&lt;00:00, 22.4kB/s]"
      }
     },
     "b31084903522470ab45178475d6e9db3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e2abb32981274adbad2bb0d68a1dfe51",
       "style": "IPY_MODEL_05f2eb9e728c42de9ec82edfead8b1a6",
       "value": "spiece.model: 100%"
      }
     },
     "b718e89c840746d29b049c2d4e1ebd12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bbf340a4b8104ff69e6d492b73b96894": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8737678c87714c3089e627a1b3cae43d",
       "style": "IPY_MODEL_17ac323f7fe04d57a090a4e894d17928",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "bc09a7b669f84fe6bdf9f8520243fefa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c231f4bed0564121acf45efccea5d786": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_07a0a776539e40649dd4523c25cd1753",
       "style": "IPY_MODEL_79e8cd1c11e843cd8ceeb381a620e89e",
       "value": " 242M/242M [00:18&lt;00:00, 13.0MB/s]"
      }
     },
     "c75b2ac6ed7b478fb020570247253a5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c7c0f93631e1452eaa81d387f138e81a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9e5e5c92ec4a4461b755072cde8ef886",
       "style": "IPY_MODEL_c7c65474319d4792a20801a4c560ecd5",
       "value": " 2.20k/2.20k [00:00&lt;00:00, 440kB/s]"
      }
     },
     "c7c65474319d4792a20801a4c560ecd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c97ecb65ef344bc787ac744fd149a139": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8301a3a0c8344cd1a4f156ffc0e084ca",
        "IPY_MODEL_f4f846db9c4b4819b786eab43cbf845c",
        "IPY_MODEL_91b2fb36dcea444db6a0d86cd5a2c747"
       ],
       "layout": "IPY_MODEL_d32f47d1726c4ec0be1338f48cf7433e"
      }
     },
     "ccdd79059c2b4021ae5585e71be7e1b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cfd8ab4f693d4c03a55bdae92acfbeb8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d269900a34164b108a7d79b84984d541": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d32f47d1726c4ec0be1338f48cf7433e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d46bf932a24b4a57ad99e24690749b96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e2abb32981274adbad2bb0d68a1dfe51": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e5fff6e9c5ad42c98f714ab1c074cc39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e8830a7403a84bb88ccb31d88ff525f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ea25ca9fe3864b289010a6ff57ef2fd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_139513e5cc36491bbea6cbfc3bfa796e",
        "IPY_MODEL_070dc19b2e384778aada666e56a18e68",
        "IPY_MODEL_c7c0f93631e1452eaa81d387f138e81a"
       ],
       "layout": "IPY_MODEL_d46bf932a24b4a57ad99e24690749b96"
      }
     },
     "ed28c08b2b824409b2fe06a2fe7b4ec4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee43abee1219410e9d14f1ad94e95eee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef5dea833d9844298ee4766615390c92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f1b0faf2fe5b4b4596ed9c662e6d10cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f4f846db9c4b4819b786eab43cbf845c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_ed28c08b2b824409b2fe06a2fe7b4ec4",
       "max": 2422193,
       "style": "IPY_MODEL_fdf1635f78b648eeb261d279e44890f2",
       "value": 2422193
      }
     },
     "fc28858b8f42414d960b43789b52043f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_11546140f2b14e50a2cc32104a16a36e",
       "style": "IPY_MODEL_c75b2ac6ed7b478fb020570247253a5b",
       "value": " 1.49k/1.49k [00:00&lt;00:00, 299kB/s]"
      }
     },
     "fdf1635f78b648eeb261d279e44890f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
