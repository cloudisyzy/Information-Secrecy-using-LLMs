{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "d:\\Software\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# Assuming helper functions like masking, add_noise_with_snr, etc., are defined here\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "# Original imports and setup\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from utils import *  # Assuming helper functions like masking, add_noise_with_snr, etc., are defined here\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Automatically choose (prefer NVIDIA GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify model name\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Specify model name for the summarizer\n",
    "summarizer_name = \"Falconsai/text_summarization\"\n",
    "tokenizer_sum = T5Tokenizer.from_pretrained(summarizer_name)\n",
    "summarizer = T5ForConditionalGeneration.from_pretrained(summarizer_name).to(device)\n",
    "\n",
    "# File path for the JSONL file\n",
    "file_path = r\"E:\\info_project\\Information-Secrecy-using-LLMs\\dataset\\3_samples.jsonl\"  # Replace with your actual file path\n",
    "texts = []\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        texts.append(data[\"text\"])\n",
    "print(texts)\n",
    "# Define parameters\n",
    "nr_rounds = 5\n",
    "snr_range = list(range(-10, 40, 5))\n",
    "\n",
    "# Storage for results\n",
    "all_cs_list_mean = []\n",
    "all_cs_list_lower = []\n",
    "all_cs_list_upper = []\n",
    "all_mi_list_mean = []\n",
    "all_mi_list_lower = []\n",
    "all_mi_list_upper = []\n",
    "\n",
    "# Process each text sample\n",
    "for text_index, text in enumerate(tqdm(texts, desc=\"Processing Samples\")):\n",
    "    input_text_ls = [masking(text, 4 / len(text)) for _ in range(nr_rounds)]\n",
    "\n",
    "    # Generate baseline summaries and embeddings for the current text\n",
    "    em_baseline_summary = []\n",
    "    for i in tqdm(range(nr_rounds), desc=f\"Baseline Processing for Sample {text_index + 1}\"):\n",
    "        input_ids = tokenizer_bart(input_text_ls[i], return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.model.encoder(input_ids=input_ids)\n",
    "\n",
    "        baseline_outputs = model.generate(input_ids=None, encoder_outputs=encoder_outputs, max_length=300, min_length=100,\n",
    "                                            num_beams=15, do_sample=True, temperature=0.15, early_stopping=True)\n",
    "        baseline_text = tokenizer_bart.decode(baseline_outputs[0], skip_special_tokens=True)\n",
    "        baseline_ids = tokenizer_sum(baseline_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            baseline_encoder_outputs = summarizer.encoder(input_ids=baseline_ids)\n",
    "        baseline_sum_output = summarizer.generate(input_ids=None, encoder_outputs=baseline_encoder_outputs, max_length=70,\n",
    "                                                    output_hidden_states=True, return_dict_in_generate=True,\n",
    "                                                    do_sample=True, temperature=0.1)\n",
    "        em_baseline_summary.append(extract_hidden_states(baseline_sum_output.decoder_hidden_states))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process noisy embeddings and calculate metrics for the current text\n",
    "    cs_list_mean = []\n",
    "    cs_list_lower = []\n",
    "    cs_list_upper = []\n",
    "    mi_list_mean = []\n",
    "    mi_list_lower = []\n",
    "    mi_list_upper = []\n",
    "\n",
    "    for target_snr in tqdm(snr_range, desc=f\"SNR Range for Sample {text_index + 1}\"):\n",
    "        cs_list_texts = []\n",
    "        mi_list_texts = []\n",
    "        for i in range(nr_rounds):\n",
    "            input_ids = tokenizer_bart(input_text_ls[i], return_tensors=\"pt\").input_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                encoder_outputs = model.model.encoder(input_ids=input_ids)\n",
    "\n",
    "            noisy_encoder_output = add_noise_with_snr(encoder_outputs.last_hidden_state, \"gaussian\", target_snr, 0.4, 0.4)\n",
    "            modified_encoder_outputs = BaseModelOutput(last_hidden_state=noisy_encoder_output)\n",
    "\n",
    "            noisy_outputs = model.generate(input_ids=None, encoder_outputs=modified_encoder_outputs, max_length=300, min_length=100,\n",
    "                                            num_beams=15, do_sample=True, temperature=0.15, early_stopping=True)\n",
    "            noisy_text = tokenizer_bart.decode(noisy_outputs[0], skip_special_tokens=True)\n",
    "            noisy_ids = tokenizer_sum(noisy_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            with torch.no_grad():\n",
    "                noisy_encoder_outputs = summarizer.encoder(input_ids=noisy_ids)\n",
    "            noisy_sum_output = summarizer.generate(input_ids=None, encoder_outputs=noisy_encoder_outputs, max_length=70,\n",
    "                                                    output_hidden_states=True, return_dict_in_generate=True,\n",
    "                                                    do_sample=True, temperature=0.1)\n",
    "            em_noisy_summary = extract_hidden_states(noisy_sum_output.decoder_hidden_states)\n",
    "\n",
    "            # Align tensors\n",
    "            pad_em_baseline_summary, pad_em_noisy_summary = align_tensors(em_baseline_summary[i], em_noisy_summary)\n",
    "\n",
    "            # Calculate metrics\n",
    "            cs_list_texts.append(F.cosine_similarity(pad_em_baseline_summary, pad_em_noisy_summary, dim=1).mean().item())\n",
    "            em_noisy_summary_np = em_noisy_summary.cpu().numpy()\n",
    "            em_baseline_summary_np = em_baseline_summary[i].cpu().numpy()\n",
    "            mi_list_texts.append(ksg(em_baseline_summary_np, em_noisy_summary_np))\n",
    "\n",
    "        # Aggregate results for the current SNR\n",
    "        cs_list_mean.append(np.mean(cs_list_texts))\n",
    "        mi_list_mean.append(np.mean(mi_list_texts))\n",
    "\n",
    "        z = 1.96  # For 95% confidence interval\n",
    "        cs_list_std = np.std(cs_list_texts, axis=0)\n",
    "        margin_of_error = z * (cs_list_std / np.sqrt(nr_rounds))\n",
    "        cs_list_lower.append(cs_list_mean[-1] - margin_of_error)\n",
    "        cs_list_upper.append(cs_list_mean[-1] + margin_of_error)\n",
    "\n",
    "        mi_list_std = np.std(mi_list_texts, axis=0)\n",
    "        margin_of_error = z * (mi_list_std / np.sqrt(nr_rounds))\n",
    "        mi_list_lower.append(mi_list_mean[-1] - margin_of_error)\n",
    "        mi_list_upper.append(mi_list_mean[-1] + margin_of_error)\n",
    "\n",
    "    # Store results for the current text sample\n",
    "    all_cs_list_mean.append(cs_list_mean)\n",
    "    all_cs_list_lower.append(cs_list_lower)\n",
    "    all_cs_list_upper.append(cs_list_upper)\n",
    "    all_mi_list_mean.append(mi_list_mean)\n",
    "    all_mi_list_lower.append(mi_list_lower)\n",
    "    all_mi_list_upper.append(mi_list_upper)\n",
    "\n",
    "# Plot results for all text samples\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(len(texts)):\n",
    "    plt.plot(snr_range, all_cs_list_mean[i], label=f\"Text Sample {i + 1}\")\n",
    "    plt.fill_between(snr_range, all_cs_list_lower[i], all_cs_list_upper[i], alpha=0.2)\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarity vs SNR for Multiple Text Samples\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(len(texts)):\n",
    "    plt.plot(snr_range, all_mi_list_mean[i], label=f\"Text Sample {i + 1}\")\n",
    "    plt.fill_between(snr_range, all_mi_list_lower[i], all_mi_list_upper[i], alpha=0.2)\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Mutual Information\")\n",
    "plt.title(\"Mutual Information vs SNR for Multiple Text Samples\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
