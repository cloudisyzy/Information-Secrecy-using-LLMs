{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b59edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria']\n",
      "Memory Allocated: 1558.20 MiB\n",
      "Memory Reserved: 3162.00 MiB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0).cuda()\n",
    "tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Example phrase\n",
    "example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "batch = tok(example_english_phrase, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate output\n",
    "generated_ids = model.generate(batch[\"input_ids\"])\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tok.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(decoded_output)\n",
    "\n",
    "\n",
    "memory_allocated_mib = torch.cuda.memory_allocated() / 1024 ** 2\n",
    "memory_reserved_mib = torch.cuda.memory_reserved() / 1024 ** 2\n",
    "\n",
    "print(f\"Memory Allocated: {memory_allocated_mib:.2f} MiB\")\n",
    "print(f\"Memory Reserved: {memory_reserved_mib:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ec22ce",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffd8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Allocated by the Model: 0.00 MiB\n",
      "Memory Reserved by the Model: 1554.00 MiB\n",
      "['UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Measure memory usage before loading the model\n",
    "initial_memory_allocated = torch.cuda.memory_allocated() / 1024 ** 2\n",
    "initial_memory_reserved = torch.cuda.memory_reserved() / 1024 ** 2\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0).cuda()\n",
    "tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "\n",
    "final_memory_allocated = torch.cuda.memory_allocated() / 1024 ** 2\n",
    "final_memory_reserved = torch.cuda.memory_reserved() / 1024 ** 2\n",
    "\n",
    "allocated_diff = final_memory_allocated - initial_memory_allocated\n",
    "reserved_diff = final_memory_reserved - initial_memory_reserved\n",
    "\n",
    "print(f\"Memory Allocated by the Model: {allocated_diff:.2f} MiB\")\n",
    "print(f\"Memory Reserved by the Model: {reserved_diff:.2f} MiB\")\n",
    "\n",
    "# Example phrase and generation\n",
    "example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "batch = tok(example_english_phrase, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate output\n",
    "generated_ids = model.generate(batch[\"input_ids\"])\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tok.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5fe571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'UN Chief Says There Is No Plan to Stop Chemical Weapons'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "ARTICLE = \"\"\" \n",
    "UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=15, min_length=10, do_sample=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a981e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
