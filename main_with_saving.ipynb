{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d910d-b318-41ef-9b9a-96533b57f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc322030-98c5-49e0-b021-0474bb1fdd0d",
   "metadata": {},
   "source": [
    "## TODO: run the code for num_beam=1, but with \"Please Summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780608da-5832-4bbb-8a94-2d608e4c315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically choose (prefer NVIDIA GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify model name\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer_bart = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Specify model name\n",
    "summarizer_name = \"Falconsai/text_summarization\"\n",
    "# summarizer_name = \"google/flan-t5-large\"\n",
    "tokenizer_sum = T5Tokenizer.from_pretrained(summarizer_name)\n",
    "summarizer = T5ForConditionalGeneration.from_pretrained(summarizer_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc13d6-df59-4150-af37-4cc67368687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"\n",
    "Beginners BBQ Class Taking Place in Missoula! \n",
    "Do you want to get better at making delicious BBQ?\n",
    "You will have the opportunity, put this on your calendar now. \n",
    "Thursday, September 22nd join World Class BBQ Champion, Tony Balay from Lonestar Smoke Rangers. \n",
    "He will be teaching a beginner level class for everyone who wants to get better with their culinary skills. \n",
    "He will teach you everything you need to know to compete in a KCBS BBQ competition, including techniques, recipes, timelines, meat selection and trimming, plus smoker and fire information. \n",
    "The cost to be in the class is $35 per person, and for spectators it is free. \n",
    "Included in the cost will be either a t-shirt or apron and you will be tasting samples of each meat that is prepared.\n",
    "\"\"\".replace(\"\\n\", \"\")\n",
    "\n",
    "# original_text = \"\"\"\n",
    "# Sport is an important part of Australian culture because the climate is good for outdoor \n",
    "# activities. 23.5% Australians over the age of 15 regularly take part in organised sporting activities. \n",
    "# In international sports, Australia has very strong teams in cricket, hockey, netball, rugby league and \n",
    "# rugby union, and performs well in cycling, rowing and swimming. Local popular sports include \n",
    "# Australian Rules Football, horse racing, soccer and motor racing. Australia has participated \n",
    "# in every summer Olympic Games since 1896, and every Commonwealth Games. Australia has hosted \n",
    "# the 1956 and 2000 Summer Olympics, and has ranked in the top five medal-winners since 2000. A\n",
    "# ustralia has also hosted the 1938, 1962, 1982 and 2006 Commonwealth Games and are to host the \n",
    "# 2018 Commonwealth Games. Other major international events held regularly in Australia include \n",
    "# the Australian Open, one of the four Grand Slam tennis tournaments, annual international cricket \n",
    "# matches and the Formula One Australian Grand Prix. Corporate and government sponsorship of many \n",
    "# sports and elite athletes is common in Australia. Televised sport is popular; some of the \n",
    "# highest-rated television programs include the Summer Olympic Games and the grand finals of \n",
    "# local and international football competitions.\n",
    "# \"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782dd33-c919-40ec-82ae-5bf409891f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_ls = []\n",
    "nr_rounds = 10\n",
    "\n",
    "for _ in range(nr_rounds):\n",
    "    input_text_ls.append(masking(original_text, 4/130))\n",
    "\n",
    "original_ids = tokenizer_sum('Please summarize: ' + original_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_encoder_outputs = summarizer.encoder(input_ids=original_ids)\n",
    "    \n",
    "original_sum_output = summarizer.generate(input_ids=None, encoder_outputs=original_encoder_outputs, max_length=70, output_hidden_states=True,\n",
    "                                        return_dict_in_generate=True, num_beams=1, do_sample=True, temperature=0.1)\n",
    "original_summary = tokenizer_sum.decode(original_sum_output.sequences[0], skip_special_tokens=True)\n",
    "print(original_summary)\n",
    "em_original_summary = extract_hidden_states(original_sum_output.decoder_hidden_states)\n",
    "\n",
    "em_baseline_summary = []\n",
    "\n",
    "for i in tqdm(range(nr_rounds)):\n",
    "    input_ids = tokenizer_bart(input_text_ls[i], return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.model.encoder(input_ids=input_ids)\n",
    "        \n",
    "    # first LLM\n",
    "    baseline_outputs = model.generate(input_ids=None, encoder_outputs=encoder_outputs, max_length=300, min_length=100, \n",
    "                                    num_beams=15, do_sample=True, temperature=0.15, early_stopping=True)\n",
    "    baseline_text = tokenizer_bart.decode(baseline_outputs[0], skip_special_tokens=True)\n",
    "    # print(baseline_text)\n",
    "\n",
    "    # \n",
    "    baseline_ids = tokenizer_sum('Please summarize: ' + baseline_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        baseline_encoder_outputs = summarizer.encoder(input_ids=baseline_ids)\n",
    "        \n",
    "    baseline_sum_output = summarizer.generate(input_ids=None, encoder_outputs=baseline_encoder_outputs, max_length=70, output_hidden_states=True,\n",
    "                                            return_dict_in_generate=True, num_beams=1, do_sample=True, temperature=0.1)\n",
    "    # , output_scores=True\n",
    "    # beam_indices = baseline_sum_output.beam_indices\n",
    "    # print(beam_scores)\n",
    "    \n",
    "    baseline_summary = tokenizer_sum.decode(baseline_sum_output.sequences[0], skip_special_tokens=True)\n",
    "    print(baseline_summary)\n",
    "    em_baseline_summary.append(extract_hidden_states(baseline_sum_output.decoder_hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e7ee8-785e-406e-b81f-f318a743e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SNR range\n",
    "# snr_range = list(range(-20, 21, 6)) + list(range(22, 55, 5)) + list(range(64, 101, 6)) \n",
    "# snr_range = list(range(-3, 20, 4)) \n",
    "snr_range = list(range(-10, 41,20))\n",
    "# snr_range = torch.linspace(0.8, 0, 20)\n",
    "cs_list_mean = []\n",
    "cs_list_lower = []\n",
    "cs_list_upper = []\n",
    "mi_list_mean = []\n",
    "mi_list_lower = []\n",
    "mi_list_upper = []\n",
    "\n",
    "#define noise type \"gaussian\", 'dropout' and  'saltpepper'\n",
    "noise_type = \"gaussian\"\n",
    "for target_snr in tqdm(snr_range):\n",
    "    cs_list_texts = []\n",
    "    mi_list_texts = []\n",
    "    for i in tqdm(range(nr_rounds)):\n",
    "        input_ids = tokenizer_bart(input_text_ls[i], return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = model.model.encoder(input_ids=input_ids)\n",
    "        \n",
    "        # add noise\n",
    "        noisy_encoder_output = add_noise_with_snr(\n",
    "            encoder_output = encoder_outputs.last_hidden_state,\n",
    "            noise_type = noise_type,\n",
    "            target_snr_db = target_snr,\n",
    "            dropout_rate = 0,\n",
    "            sp_thresh = 0\n",
    "        )\n",
    "        modified_encoder_outputs = BaseModelOutput(last_hidden_state=noisy_encoder_output)\n",
    "        \n",
    "        # first LLM\n",
    "        noisy_outputs = model.generate(input_ids=None, encoder_outputs=modified_encoder_outputs, max_length=300, min_length=100, \n",
    "                                    num_beams=15, do_sample=True, temperature=0.15, early_stopping=True)\n",
    "        noisy_text = tokenizer_bart.decode(noisy_outputs[0], skip_special_tokens=True)\n",
    "        #print(noisy_text)\n",
    "    \n",
    "        # second LLM\n",
    "        noisy_ids = tokenizer_sum('Please summarize: ' + noisy_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            noisy_encoder_outputs = summarizer.encoder(input_ids=noisy_ids)\n",
    "        noisy_sum_output = summarizer.generate(input_ids=None, encoder_outputs=noisy_encoder_outputs, max_length=70, output_hidden_states=True,\n",
    "                                            return_dict_in_generate=True, do_sample=True, num_beams=1, temperature=0.1)\n",
    "        noisy_summary = tokenizer_sum.decode(noisy_sum_output.sequences[0], skip_special_tokens=True)\n",
    "        #print(noisy_summary)\n",
    "        \n",
    "        \n",
    "        # get embeddings\n",
    "        em_noisy_summary = extract_hidden_states(noisy_sum_output.decoder_hidden_states)\n",
    "        \n",
    "        # pad each embedding with embedding of baseline\n",
    "        pad_em_baseline_summary, pad_em_noisy_summary = align_tensors(em_baseline_summary[i], em_noisy_summary)\n",
    "        \n",
    "        # calculate cs\n",
    "        cs_list_texts.append(F.cosine_similarity(pad_em_baseline_summary, pad_em_noisy_summary, dim=1).mean().item())\n",
    "    \n",
    "        # calculate mi\n",
    "        em_noisy_summary_np = em_noisy_summary.cpu().numpy()\n",
    "        em_baseline_summary_np = em_baseline_summary[i].cpu().numpy()\n",
    "        mi_list_texts.append(ksg(em_baseline_summary_np, em_noisy_summary_np))\n",
    "    \n",
    "    # for each snr calculate mean cs and mi\n",
    "    cs_list_mean.append(np.mean(cs_list_texts))\n",
    "    mi_list_mean.append(np.mean(mi_list_texts))\n",
    "    \n",
    "\n",
    "    z = 1.96  # For 95% confidence level\n",
    "    cs_list_std = np.std(cs_list_texts, axis=0)\n",
    "    margin_of_error = z * (cs_list_std / np.sqrt(nr_rounds))\n",
    "    cs_list_lower.append(cs_list_mean[-1] - margin_of_error)\n",
    "    cs_list_upper.append(cs_list_mean[-1] + margin_of_error)\n",
    "    # print(cs_list_lower)\n",
    "\n",
    "    mi_list_std = np.std(mi_list_texts, axis=0)\n",
    "    margin_of_error = z * (mi_list_std / np.sqrt(nr_rounds))\n",
    "    mi_list_lower.append(mi_list_mean[-1] - margin_of_error)\n",
    "    mi_list_upper.append(mi_list_mean[-1] + margin_of_error)\n",
    "    \n",
    "cs_base_orig_ls = []\n",
    "mi_base_orig_ls = []\n",
    "for i in range(nr_rounds):\n",
    "    pad_em_baseline_summary, pad_em_original_summary = align_tensors(em_baseline_summary[i], em_original_summary)\n",
    "    cs_base_orig_ls.append(F.cosine_similarity(pad_em_baseline_summary, pad_em_original_summary, dim=1).mean().item())\n",
    "\n",
    "    em_original_summary_np = em_original_summary.cpu().numpy()\n",
    "    em_baseline_summary_np = em_baseline_summary[i].cpu().numpy()\n",
    "\n",
    "    mi_base_orig_ls.append(ksg(em_baseline_summary_np, em_original_summary_np))\n",
    "    \n",
    "cs_base_orig_mean = np.mean(cs_base_orig_ls)\n",
    "mi_base_orig_mean = np.mean(mi_base_orig_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819a0a0-906a-4327-9ad0-46001a89b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "plt.plot(snr_range, cs_list_mean, marker='o')\n",
    "plt.axhline(y=cs_base_orig_mean, color='r', linestyle='--', label='Cosine Similarity between the baseline and the original')\n",
    "plt.fill_between(snr_range, cs_list_lower, cs_list_upper, color='b', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.ylim([0.0,1.1])\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarity of Baseline and Noisy Summary\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "plt.plot(snr_range, mi_list_mean, marker='o')\n",
    "plt.axhline(y=mi_base_orig_mean, color='r', linestyle='--', label='Mutual Information between the baseline and the original')\n",
    "plt.fill_between(snr_range, mi_list_lower, mi_list_upper, color='b', alpha=0.2, label='95% Confidence Interval')\n",
    "plt.ylim([0.0,5.1])\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"MI similarities of Baseline\")\n",
    "plt.title(\"MI similarities of Baseline and Noisy Summary\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec87e4-d920-43af-b47d-fd8bb5fa2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = os.path.join(os.getcwd(), noise_type)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "text_path = os.path.join(output_dir, f\"masked_text_{timestamp}.jsonl\")\n",
    "\n",
    "#Save the input_text\n",
    "with open(text_path, \"w\") as f:\n",
    "    for text in input_text_ls:\n",
    "        json_line = json.dumps({\"masked_text\": text})\n",
    "        f.write(json_line + \"\\n\")\n",
    "\n",
    "#Save the cs and mi \n",
    "# Save data for the first plot (Cosine Similarity)\n",
    "cosine_similarity_data = pd.DataFrame({\n",
    "    \"SNR (dB)\": snr_range,\n",
    "    \"Cosine Similarity Mean\": cs_list_mean,\n",
    "    \"95% CI Lower Bound\": cs_list_lower,\n",
    "    \"95% CI Upper Bound\": cs_list_upper,\n",
    "    \"Baseline-Original Cosine Similarity\": [cs_base_orig_mean] * len(snr_range),\n",
    "})\n",
    "\n",
    "cosine_similarity_csv_path = os.path.join(output_dir, f\"cs_{timestamp}.csv\")\n",
    "cosine_similarity_data.to_csv(cosine_similarity_csv_path, index=False)\n",
    "\n",
    "# Save data for the second plot (Mutual Information)\n",
    "mi_similarity_data = pd.DataFrame({\n",
    "    \"SNR (dB)\": snr_range,\n",
    "    \"Mutual Information Mean\": mi_list_mean,\n",
    "    \"95% CI Lower Bound\": mi_list_lower,\n",
    "    \"95% CI Upper Bound\": mi_list_upper,\n",
    "    \"Baseline-Original Mutual Information\": [mi_base_orig_mean] * len(snr_range),\n",
    "})\n",
    "\n",
    "mi_similarity_csv_path = os.path.join(output_dir, f\"mi_{timestamp}.csv\")\n",
    "mi_similarity_data.to_csv(mi_similarity_csv_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Input masked text data saved to: {text_path}\")\n",
    "print(f\"Cosine Similarity data saved to: {cosine_similarity_csv_path}\")\n",
    "print(f\"Mutual Information data saved to: {mi_similarity_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e763ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replot graph using saved data\n",
    "\n",
    "# Load data\n",
    "cosine_similarity_data = pd.read_csv(cosine_similarity_csv_path)\n",
    "mi_similarity_data = pd.read_csv(mi_similarity_csv_path)\n",
    "\n",
    "# Replot Cosine Similarity graph\n",
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "plt.plot(cosine_similarity_data[\"SNR (dB)\"], cosine_similarity_data[\"Cosine Similarity Mean\"], marker='o', label=\"Mean\")\n",
    "plt.axhline(\n",
    "    y=cosine_similarity_data[\"Baseline-Original Cosine Similarity\"].iloc[0],\n",
    "    color='r',\n",
    "    linestyle='--',\n",
    "    label='Baseline-Original Cosine Similarity'\n",
    ")\n",
    "plt.fill_between(\n",
    "    cosine_similarity_data[\"SNR (dB)\"],\n",
    "    cosine_similarity_data[\"95% CI Lower Bound\"],\n",
    "    cosine_similarity_data[\"95% CI Upper Bound\"],\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='95% Confidence Interval'\n",
    ")\n",
    "plt.ylim([0.0, 1.1])\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.title(\"Cosine Similarity of Baseline and Noisy Summary\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Replot Mutual Information graph\n",
    "plt.figure(figsize=(20, 6), dpi=300)\n",
    "plt.plot(mi_similarity_data[\"SNR (dB)\"], mi_similarity_data[\"Mutual Information Mean\"], marker='o', label=\"Mean\")\n",
    "plt.axhline(\n",
    "    y=mi_similarity_data[\"Baseline-Original Mutual Information\"].iloc[0],\n",
    "    color='r',\n",
    "    linestyle='--',\n",
    "    label='Baseline-Original Mutual Information'\n",
    ")\n",
    "plt.fill_between(\n",
    "    mi_similarity_data[\"SNR (dB)\"],\n",
    "    mi_similarity_data[\"95% CI Lower Bound\"],\n",
    "    mi_similarity_data[\"95% CI Upper Bound\"],\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='95% Confidence Interval'\n",
    ")\n",
    "plt.ylim([0.0, 5.1])\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Mutual Information\")\n",
    "plt.title(\"Mutual Information of Baseline and Noisy Summary\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
